# -*- coding: utf-8 -*-
"""seperate_encoding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b7Fzarkd752KbAph81bloPI1iaM-1-gL
"""

import requests
from bs4 import BeautifulSoup
import re
import time
import sys

data = ""
for line in sys.stdin:
  data += line

print(data)

#splitting the document into their different websites
websites = data.split("</doc>")

len(websites)

#getting the encoding of the site through meta tag
def get_encoding_by_tag(soup):
    if soup and soup.meta:
        encod = soup.meta.get('charset')
        if encod == None:
            encod = soup.meta.get('content-type')
            if encod == None:
                content = soup.meta.get('content')
                match = re.search('charset=(.*)', content)
                if match:
                    encod = match.group(1)
                else:
                    return "N/A"
    else:
        return "N/A"
    return encod

#getting the encoding by tag or through Unicode Dammit
def get_encoding(url):
  try:
    source = requests.get(url)
  except:
    print("Link broken for " + url, file=sys.stderr)
    return "N/A"

  try:
    soup = BeautifulSoup(source.content, 'lxml')
    encoding = get_encoding_by_tag(soup)
    if (encoding == "N/A"): #fails to retrieve encoding using tag, try Unicode Dammit
      encoding = soup.original_encoding
    return encoding
  except:
    print("Can not make soup for " + url, file=sys.stderr)
    return "N/A"

utf = []
other = []
for site in websites[0:2]:
  #remove empty lines
  sentences = list(filter(lambda x: True if x != "" else False, site.split("\n")))

  #url is contained in the first sentence of each segment
  header = sentences[0]
  url = header[header.find("url=")+5: header.find("warc-file")-2]

  encoding = get_encoding(url)

  if (encoding == "N/A"): #something went wrong with get_encoding
    continue
  elif (encoding == "utf-8" or encoding == "UTF-8"): #add to training data for utf-8
    utf += sentences
  else: #add to training data for other
    other += sentences

print("---UTF-8---")
for item in utf:
  print(item)

print("---OTHER---")
for item in other:
  print(other)